---
title: "LeMondeScrapper"
author: "Felix Martel"
date: "21 novembre 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Build HTTP request

```{r}
library(lubridate)
library(httr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(readr)
library(tidyverse)
library(rvest)

# 'http://abonnes.lemonde.fr/recherche/?keywords=internet&page_num=1&operator=and&exclude_keywords=&qt=recherche_texte_titre&author=&period=custom_date&start_day=01&start_month=01&start_year=1944&end_day=22&end_month=11&end_year=2017&sort=asc'

# Init parameters
base_url <- 'http://abonnes.lemonde.fr/recherche/?'
keys <- c('keywords', 'page_num', 'operator', 'exclude_keywords', 'qt', 'author', 'period', 'start_day', 'start_month', 'start_year', 'end_day', 'end_month', 'end_year', 'sort')
params <- rep('', length(keys))
names(params) <- keys

# Base values
params['page_num'] = 1
params['operator'] = 'and'
params['qt'] = 'recherche_titre' # 'recherche_texte_titre'
params['period'] = 'custom_date'
params['sort'] = 'asc'

# Set values
## Dates
start_date <- ymd('2016-12-01')
end_date <- ymd('2017-07-31')
params['start_day'] <- day(start_date)
params['start_month'] <- month(start_date)
params['start_year'] <- year(start_date)

params['end_day'] <- day(end_date)
params['end_month'] <- month(end_date)
params['end_year'] <- year(end_date)

## Keywords
keyword <- 'macron'
params['keywords'] <- keyword

build_request <- function(url, keys, values){
  parameters = paste0(keys, rep('=', length(keys)), values)
  parameters_string <- paste(parameters, sep='', collapse='&')
  return(paste(url, parameters_string, sep=''))
}

create_date <- function(params, type="start"){
  return(paste(params[paste(type, "_day", sep="")], params[paste(type, "_month", sep="")], params[paste(type, "_year", sep="")], sep="/"))
}

url <- build_request(base_url, keys, params)
```

```{r}

```

# Crawler

```{r}
login <- function(){
  # Params
  user <- 'felix.martel@gmx.com'
  pwd <- 'lettre'
  
  # Create session
  s <- html_session('https://secure.lemonde.fr/sfuser/connexion')
  
  # Sign in
  signin_form <- html_form(s)[[3]]
  signin_form <- set_values(signin_form, 'connection[mail]' = user)
  signin_form <- set_values(signin_form, 'connection[password]' = pwd)
  res <- submit_form(s, signin_form)
  return(res)
}

get_nodes <- function(s, node) {
  return(read_html(s) %>% html_nodes(node) %>% html_text())
}

get_titles <- function(s) {
  read_tags <- function(sel){
    return(read_html(s) %>% html_nodes(sel) %>% html_text())
  }
  read_href <- function(sel){
    return(read_html(s) %>% html_nodes(sel) %>% html_attr(name="href"))
  }
  
  titles <- read_tags("article h3 > a")
  dates <- read_tags("article .resultat span.signature")
  hrefs <- read_href("article h3 > a")
  
  df <- data.frame(
    title=titles, 
    date=dates, 
    href=hrefs
  )
  return(df)
}

tag_exists <- function(s, tag) {
  tag_list <- html_nodes(s, css=tag)
  return(length(tag_list[1]) > 0)
}


getData <- function(session, params){
  url <- build_request(base_url, keys, params)
  #print(paste("Going to '", url, "'...", sep=""))
  s <- jump_to(session, url)
  titles <- data.frame(title=character(), date=character(), href=character())
  
  # Pages
  start_page <- params['page_num']
  page <- 1
  nb_results <- as.integer(html_nodes(s, css="section.resultats_recherche > p:first-of-type strong:first-of-type") %>% html_text())
  max_page <- as.integer(nb_results / 10) + 1
  
  # Print parameters and number of results
  print(paste(nb_results, " results for query '", params["keywords"], "' between ", create_date(params, "start"), " and ", create_date(params, "end"), sep=""))
  
  for (i in 1:max_page){
    params['page_num'] = i
    print(paste(params["keywords"], i))
    url <- build_request(base_url, keys, params)
    s <- jump_to(s, url)
    chunk <- tryCatch({
      get_titles(s)
    }, error = function(e){
      print(paste('ERROR ON LINE', i))
      data.frame(title=character(), date=character(), href=character())
    })
    titles <- rbind(titles, chunk)
  }
  
  titles$href <- paste("http://abonnes.lemonde.fr", titles$href, sep="")
  titles$date <- sapply(strsplit(as.character(titles$date), "| ", fixed=T), '[', 2)
  titles$date <- as.Date(titles$date, format="%d %B %Y")
  return(titles)
}

#s <- login()
#df <- getData(s, params)

words <- c('presidentielle', 'macron')


getAll <- function(words, params){
  #words <- gsub(" ", "+", words)
  s <- login()
  df <-  data.frame(
    title=character(), 
    date=character(), 
    href=character()
  )
  for (w in words){
    print(paste('Retrieving', w))
    params['keywords'] <- w
    res <- tryCatch({
      getData(s, params)
    }, error = function(e){
      print(paste('ERROR ON WORD', w))
      data.frame(
        title=character(), 
        date=character(), 
        href=character()
      )
    })
    df <- rbind(df, res)
  }
  return(df)
}

#titles <- getAll(words, params)
#write.table(titles, file="./data/lemonde.csv", sep=";")
```

## Récupérer l'URL des articles

```{r}
# Get titles
tfilters <- read.csv('./data/titles_filter.csv', encoding="UTF-8")
tfilters <- as.character(unlist(tfilters, use.names = F))
nw <- length(tfilters)
tfilters <- tfilters[2:nw]

df <- getAll(tfilters, params)
write.table(titles, file="./data/lemonde_all.csv", sep=";")
```


# Récuper le texte des articles

# Utils functions

```{r}
filter_corpus <- function(h){
  # Convertir le texte en minuscule
  x <- tm_map(h, content_transformer(tolower))
  # Supprimer les mots vides
  x <- tm_map(x, removeWords, stopwords("french"))
  # Supprimer votre propre liste de mots non désirés
  #x <- tm_map(x, removeWords, c("bla"))
  # Supprimer les ponctuations
  x <- tm_map(x, removePunctuation, ucp=T)
  # Supprimer les espaces vides supplémentaires
  x <- tm_map(x, stripWhitespace)
  return(x)
}

getArticle <-function(session, href, words){
  res <- tryCatch({
    s <- jump_to(session, href)
    node <- s %>% read_html() %>%  html_node("article")
    text <- node %>% html_nodes("p, h1, h2, h3, h4, h5, h6") %>% html_text %>% stringr::str_c(sep="",collapse="\n")
    text <- filter_corpus(Corpus(VectorSource(text)))
    dtm <- TermDocumentMatrix(text)
    m <- as.matrix(dtm)
    v <- sort(rowSums(m),decreasing=TRUE)
    print(url)
    my_d <- data.frame(word = names(v),freq=v)
    return(my_d[words,"freq"])
  }, error = function(e){return(rep(NA, length(words)))})
}

relevant_words <- as.vector(read.csv("./data/relevant_words.csv", encoding = 'UTF-8', header=F)$V1)


# words_table <- data.frame(do.call(rbind,lapply(res$link[relevant_articles], FUN = function(url) scrap_lexpress_text(url,relevant_words))))
# words_table[is.na(words_table)] <- 0
# colnames(words_table) <- relevant_words
# rownames(words_table) <- res$title[relevant_articles]
# 
# save(words_table,file="lexpress_table.Rda")
```



```{r}
relevant_words <- as.vector(read.csv("./data/relevant_words.csv", encoding = 'UTF-8', header=F)$V1)
session <- login()
href <- "http://abonnes.lemonde.fr/campus/article/2016/12/01/presidentielle-2017-les-propositions-des-grandes-ecoles-pour-faire-reussir-la-jeunesse_5041290_4401467.html?xtmc=presidentielle&xtcr=1"
s <- jump_to(session, href)
node <- s %>% read_html() %>%  html_node("article")
rawtext <- node %>% html_nodes("p, h1, h2, h3, h4, h5, h6") %>% html_text %>% stringr::str_c(sep="",collapse=" ")
Encoding(rawtext) <- "UTF-8"
text <- filter_corpus(Corpus(DataframeSource(data.frame(doc_id=c(1), text=c(rawtext)))))
dtm <- TermDocumentMatrix(text)
inspect(dtm)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
print(href)
my_d <- data.frame(word = names(v),freq=v)
my_d
```


