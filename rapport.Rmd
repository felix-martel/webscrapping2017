---
title: "Analyse de la couverture médiatique de la campagne présidentielle 2017"
author: "Amina Bouchafaa - Maxime Godin - Félix Martel"
date: "07/12/2017"
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=F}
library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)

# Wordcloud
library(wordcloud)
library(RColorBrewer)

# Text mining
library(tm)

# Correspondence analysis
library(FactoMineR)
```


# Présentation du projet

Au cours de ce projet, nous nous sommes intéressés à la couverture médiatique de la campagne présidentielle française de 2017. Nous avons collecté les articles de trois titres nationaux, L'Express, Le Monde et Le Figaro sur la période du 1er décembre 2016 jusqu'au 31 juillet 2017 à l'aide d'outils de webscraping. Parallélément, nous avons établi une liste de mots d'intérêt dont nous avons suivi l'utilisation dans les articles de presse retenus. Nous mettons en oeuvre différentes techniques d'analyse et de représentation  (nuage de mots, graphiques, analyse factorielle et classification hiérarchique) en les appliquant à des données textuelles mis sous forme de table de contingence.

# Création des données

Dans cette partie, nous nous intéressons à la récupération des données à partir des sites Internet des trois titres de presse retenus dans notre étude : Le Monde, L'Express et Le Figaro. Pour chaque articles pertinents, nous avons construit une table de contingence avec une liste de mots d'intérêt.

Trois fichiers .Rmd (lemonde_scraper.Rmd, lexpress_scraper.Rmd et lefigaro_scraper.Rmd) indépendants du rapport permettent de récupérer les articles par des méthodes de webscraping puis de créer les tables de contingence avec des outils de text mining. Nous n'avons pas inclus ces étapes dans le rapport car leur temps d'éxecution ne permet de pas de reproduire les résultats rapidement. Les trois tables contenant les résultats sont contenus dans le dossier data (lemonde_words_table.Rda, lexpress_words_table.Rda et lefigaro_words_table.Rda).

## Récupération des articles

La collecte de données à partir de pages Internet est appelée webscraping. Cette technique consiste à exploiter la structure html des pages de sites Internet pour récupérer des données. Nous avons utilisé le package rvest qui permet de récupérer les élements d'une page web en utilisant des sélecteurs CSS. 

Dans un premier temps, nous avons déterminé une liste de mots-clés pour cibler les articles traitant de la campagne présidentielle. Les articles retenus étaient ceux qui contenaient un des mots-clés.

```{r, echo=F,message=F}
titles_filter <- read_csv("data/titles_filter.csv",col_names = FALSE)$X1
```


## Construction des tables de contingence

Nous avons par ailleurs déterminé une liste de mots d'intérêt dont nous souhaitions mesurer l'utilisation dans les articles retenus.

```{r, echo=F,message=F}
relevant_words <- read_csv("data/relevant_words.csv",col_names = FALSE)$X1
```

Avant de compter le nombre d'occurences de chaque mot d'intérêt dans les articles de presse récupérés, il faut réaliser une série de traitements pour transformer les mots dans une forme standard. 

On commence par transformer le texte de l'article en un objet de classe Corpus pour pouvoir ensuite applquer des fonctions du package tm, pour Text Mining Packag.

La fonction filter_corpus permet de composer une série de transformation sur un objet de classe Corpus, c'est à dire une collection de documents.

```{r, eval=T,results=F}

filter_corpus <- function(.){
  # Convertir le texte en minuscule
  tm_map(., content_transformer(tolower)) %>% 
  # Supprimer les mots vides
  tm_map(., removeWords, stopwords("french")) %>% 
  # Supprimer votre propre liste de mots non désirés
  tm_map(., removeWords, c("orang-outan,chimpanzé")) %>% 
  # Supprimer les ponctuations
  tm_map(., removePunctuation) %>% 
  # Supprimer les espaces vides supplémentaires
  tm_map(., stripWhitespace)
}
```

Ensuite, on utilise la fonction TermDocumentMatrix pour réaliser le décompte du nombre d'occurences de chaque mot dans chaque article pour en extraire la fréquence des mots d'intérêts. Nous donnons ci-dessous un exemple jouet avec trois textes philosophiques et 6 mots d'intérêt pour mieux fixer les idées.

```{r, include=F}
texts <- sapply(c(1:3),function(i) {stringr::str_c(readLines(paste("data/texts/t",i,".txt",sep="")),collapse = "\n")})
words = c("ignorance","doute","raison","force","penser","croire")
```


```{r}
words_table_maker <- function(text,words){
  text <- filter_corpus(Corpus(VectorSource(text)))
  dtm <- TermDocumentMatrix(text)
  m <- as.matrix(dtm)
  v <- sort(rowSums(m),decreasing=TRUE)
  my_d <- data.frame(word = names(v),freq=v)
  return(my_d[words,"freq"])
}

my_words_table <- data.frame(do.call(rbind,lapply(texts, 
                                               FUN = function(t) words_table_maker(t,words))))
my_words_table[is.na(my_words_table)] <- 0
colnames(my_words_table) <- words
rownames(my_words_table) <- paste("texte",c(1:length(texts)),sep=" ")
knitr::kable(my_words_table)
```

# Analyses et résultats

Nous commençons par charger les tables de contingence des trois journaux étudiés qui sont enregistrées dans le dossier data. Nous compilons les résultats dans une unique table de données.

```{r, result=FALSE}
load("data/lexpress_words_table.Rda")
load("data/lefigaro_words_table.Rda")
load("data/lemonde_words_table.Rda")


words_table <- rbind(data.frame(journal = rep("Le Figaro",nrow(lefigaro_words_table)),lefigaro_words_table),
                     data.frame(journal = rep("L'Express",nrow(lexpress_words_table)),lexpress_words_table),
                     data.frame(journal = rep("Le Monde",nrow(lemonde_words_table)),lemonde_words_table))

relevant_words = colnames(words_table)[4:ncol(words_table)]
```


## Utilisation de mots d'intérêt dans la presse

### Nuages de mots

Nous avons utilisé des nuages de mots pour représenter le vocabulaire utilisé dans les trois quotidiens au cours de chaque mois de la période retenue. En comparant les nuages de mots, on peut ainsi repérer visuellement les similarités et les différences dans les sujets abordés par ces trois titres de presse. 

```{r,eval=F}

words_hotness <- words_table %>% 
  mutate(month = as.factor(month(date)), date=NULL) %>% 
  group_by(journal,month) %>% 
  summarise_at(relevant_words,sum) %>% 
  data.frame()
for (m in levels(words_hotness$month)){
  for (j in levels(words_hotness$journal)){
        png(paste(paste("images/wordcloud",gsub(" ","",j),m,sep="_"),".png",sep=""), width=6, height=6, units="in", res=150)
        wordcloud(words = relevant_words, freq = words_hotness[words_hotness$month==m & words_hotness$journal==j,relevant_words], min.freq = 1,
          max.words=20, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
        dev.off()
  }
}
graphics.off()
```
Mois | Le Monde | Le Figaro | L'Express
:-----:|:----------:|:-----------:|:----------:
décembre |![](images/wordcloud_LeMonde_12.png){width=6cm} | ![](images/wordcloud_LeFigaro_12.png){width=6cm}  | ![](images/wordcloud_L'Express_12.png){width=6cm}
janvier |![](images/wordcloud_LeMonde_1.png){width=6cm} | ![](images/wordcloud_LeFigaro_1.png){width=6cm}  | ![](images/wordcloud_L'Express_1.png){width=6cm}
février |![](images/wordcloud_LeMonde_2.png){width=6cm} | ![](images/wordcloud_LeFigaro_2.png){width=6cm}  | ![](images/wordcloud_L'Express_2.png){width=6cm}
mars |![](images/wordcloud_LeMonde_3.png){width=6cm} | ![](images/wordcloud_LeFigaro_3.png){width=6cm}  | ![](images/wordcloud_L'Express_3.png){width=6cm}
avril |![](images/wordcloud_LeMonde_4.png){width=6cm} | ![](images/wordcloud_LeFigaro_4.png){width=6cm}  | ![](images/wordcloud_L'Express_4.png){width=6cm}
mai |![](images/wordcloud_LeMonde_5.png){width=6cm} | ![](images/wordcloud_LeFigaro_5.png){width=6cm}  | ![](images/wordcloud_L'Express_5.png){width=6cm}
juin |![](images/wordcloud_LeMonde_6.png){width=6cm} | ![](images/wordcloud_LeFigaro_6.png){width=6cm}  | ![](images/wordcloud_L'Express_6.png){width=6cm}
juillet |![](images/wordcloud_LeMonde_7.png){width=6cm} | ![](images/wordcloud_LeFigaro_7.png){width=6cm}  | ![](images/wordcloud_L'Express_7.png){width=6cm}

### Evolution temporelle lissée

## Analyse factorielle des correspondances

### Représentation graphique des articles et des mots d'intérêt

### Classification hiérarchique sur composantes principales

Le package factomineR contient une fonction permettant de réaliser une classification hiérarchique sur composantes principales à partir d'une analyse factorielle des correspondances.

#### Nuages de mots des clusters

```{r, eval=F,include=F}
#fonction qui donne les images des clusters en png
Cluster<-function(words.hcpc,name){
  nb_clusters <- length(words.hcpc$desc.var)
  for (j in 1:nb_clusters){
        png(paste(paste(paste("clusters/",name,sep=""),j,sep="_"),".png",sep=""),    width=6, height=6, units="in", res=150)
        frq<-words.hcpc$desc.var[[j]][,"Intern freq"]
        names<-names(words.hcpc$desc.var[[j]][,"Intern freq"])
      wordcloud(words = names, freq = frq , min.freq = 1,
            max.words=150, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
        dev.off()
  }
  graphics.off()
}
```

##### Tous journaux confondus

```{r, eval=F,include=F}
words_table_CA <- words_table 
non_zero_words <- names(which(words_table_CA %>% select(-journal,-title,-date) %>% colSums() >0))
words_table_CA <- cbind(words_table_CA[,c(1,2,3)],words_table_CA[,non_zero_words])
non_zero_rows <- which(words_table_CA %>% select(-journal,-title,-date) %>% rowSums() >0)
words_table_CA <- words_table_CA[non_zero_rows,]

#selection des dates avant elections
w<-words_table_CA[which(words_table_CA$date < as.Date("07-05-2017",format="%d-%m-%Y")),]
journaux<-words_table_CA[,'journal']
words_table_CA <- words_table_CA %>% select(-journal,-date,-title)

res.CA <- CA(words_table_CA,graph=F,ncp=20)
res.HCPC <- HCPC (res.CA,graph=F,nb.clust=6)
res.HCPC$data.clust[,'journal']<-journaux
#plot.HCPC(res.HCPC,invisible=c("row"))
save(res.HCPC,file="clusters/HCPC.Rda")
```
```{r,eval=F,include=F}
#clusters des trois journaux ensemble
load("clusters/HCPC.Rda")
Cluster(res.HCPC,"global_clusters")
```


<center>  
|          |          |          |
|:--------:|:--------:|:--------:|
|![](clusters/global_clusters_1.png){width=6cm} | ![](clusters/global_clusters_2.png){width=6cm} | ![](clusters/global_clusters_3.png){width=6cm}|
|    (1) gauche      |     (2) macron     |   (3) politique générale      |
| ![](clusters/global_clusters_4.png){width=6cm} | ![](clusters/global_clusters_5.png){width=6cm} | ![](clusters/global_clusters_6.png){width=6cm} |
|    (4)  travail    |    (5)  fillon    |   (6)  enquête     |


```{r,echo=F,message=F}
load("clusters/HCPC.Rda")

repart <- res.HCPC$data.clust %>% 
  select(journal,clust) %>% 
  group_by(journal,clust) %>% 
  summarise(prop = n())

poids_journaux <- repart %>% 
  select(journal,prop) %>% 
  group_by(journal) %>% 
  summarise(poids = sum(prop))

taille_clusters <- repart %>% 
  select(clust,prop) %>% 
  group_by(clust) %>% 
  summarise(taille = sum(prop)) %>% 
  mutate(taille = taille/sum(taille))

repart <- repart %>% 
  left_join(poids_journaux,by = "journal") %>% 
  mutate(prop = prop / poids) %>% 
  select(clust,journal,prop)

ggplot(repart)+
  aes(x=clust,y=prop,fill=journal)+
  geom_col()+xlab("cluster")+
  ylab("répartition")

ggplot(taille_clusters)+
  aes(x=clust,y=taille)+
  geom_col()+xlab("cluster")+
  ylab("taille")
```


</center>

##### Par journaux

```{r, eval=F,include=F}
#traitement pour HCPC
traitement_CA<-function(words_tmp){
  words_res<-words_tmp
  words_res<-words_res[which(words_res$date < as.Date("07-05-2017",format="%d-%m-%Y")),]
  words_res <- words_res %>% select(-date,-title)
  non_zero_rows <- which(words_res %>% rowSums() >0)
  words_res<-words_res[non_zero_rows,]
  non_zero_cols <- which(words_res %>% colSums() >0)
  words_res<-words_res[,non_zero_cols]
  words_res<-CA(words_res,ncp=20,graph=F)
  return (words_res)
}


lexpress_CA<-traitement_CA(lexpress_words_table)
lefigaro_CA<-traitement_CA(lefigaro_words_table)
lemonde_CA<-traitement_CA(lemonde_words_table)

lexpress_HCPC<-HCPC(lexpress_CA,nb.clust=6,graph=F)
lefigaro_HCPC<-HCPC(lefigaro_CA,nb.clust=6,graph=F)
lemonde_HCPC<-HCPC(lemonde_CA,nb.clust=6,graph=F)

Cluster(lexpress_HCPC,name="lexpress_cluster")
Cluster(lefigaro_HCPC,"lefigaro_cluster")
Cluster(lemonde_HCPC,"lemonde_cluster")
```


Cluster | Le Monde | Le Figaro | L'Express
:-----:|:----------:|:-----------:|:----------:
1 |![](clusters/lemonde_cluster_1.png){width=6cm} | ![](clusters/lefigaro_cluster_1.png){width=6cm}  | ![](clusters/lexpress_cluster_1.png){width=6cm}
2 |![](clusters/lemonde_cluster_2.png){width=6cm} | ![](clusters/lefigaro_cluster_2.png){width=6cm}  | ![](clusters/lexpress_cluster_2.png){width=6cm}
3 |![](clusters/lemonde_cluster_3.png){width=6cm} | ![](clusters/lefigaro_cluster_3.png){width=6cm}  | ![](clusters/lexpress_cluster_3.png){width=6cm}
4 |![](clusters/lemonde_cluster_4.png){width=6cm} | ![](clusters/lefigaro_cluster_4.png){width=6cm}  | ![](clusters/lexpress_cluster_4.png){width=6cm}
5 |![](clusters/lemonde_cluster_1.png){width=6cm} |  ![](clusters/lefigaro_cluster_5.png){width=6cm} | ![](clusters/lexpress_cluster_5.png){width=6cm}
6 |![](clusters/lemonde_cluster_6.png){width=6cm} | ![](clusters/lefigaro_cluster_6.png){width=6cm}  | ![](clusters/lexpress_cluster_6.png){width=6cm}
